# ğŸš€ Data Engineer Junior Project: Pipeline de IngestÃ£o e OrquestraÃ§Ã£o

Este repositÃ³rio contÃ©m uma soluÃ§Ã£o completa de Engenharia de Dados focada na construÃ§Ã£o de um **Data Lake** funcional, utilizando prÃ¡ticas de containerizaÃ§Ã£o, armazenamento de objetos e orquestraÃ§Ã£o de fluxos de trabalho.

O projeto automatiza o ciclo de vida do dado: desde a extraÃ§Ã£o bruta (Raw), passando pelo refinamento (Silver/Trusted), atÃ© a disponibilizaÃ§Ã£o em um Data Mart (Gold) pronto para anÃ¡lise.



---

## ğŸ› ï¸ Tecnologias e Ferramentas

| Ferramenta | Uso no Projeto |
| :--- | :--- |
| **Python 3.x** | Linguagem principal para processamento e lÃ³gica de scripts. |
| **Apache Airflow** | Orquestrador de workflows (DAGs) e agendamento de tarefas. |
| **MinIO** | Data Lake local compatÃ­vel com S3 para armazenamento de objetos. |
| **Docker & Compose** | ContainerizaÃ§Ã£o de toda a stack para portabilidade. |
| **Shell Scripting** | AutomaÃ§Ã£o de rotinas de setup e execuÃ§Ã£o de testes. |
| **PostgreSQL** | Banco de dados de metadados para o Airflow. |

---

## ğŸ§  Por que usamos Airflow e DAGs?

A escolha do **Apache Airflow** como orquestrador central Ã© estratÃ©gica por diversos motivos tÃ©cnicos que elevam a robustez do projeto:

1.  **DAGs (Directed Acyclic Graphs):** As tarefas sÃ£o organizadas em Grafos AcÃ­clicos Dirigidos. Isso garante que as dependÃªncias sejam respeitadas (ex: o script de `transformation.py` nunca serÃ¡ executado antes que o `extract_data.py` termine com sucesso).
2.  **IdempotÃªncia e Retentativas:** O Airflow permite configurar polÃ­ticas de *retry*. Se uma API falhar momentaneamente, a DAG tentarÃ¡ novamente de forma automÃ¡tica sem duplicar dados.
3.  **Monitoramento Centralizado:** AtravÃ©s da interface web, Ã© possÃ­vel visualizar gargalos, conferir logs detalhados (armazenados em `logs/`) e gerenciar o histÃ³rico de execuÃ§Ãµes.
4.  **Escalabilidade:** A arquitetura modular permite que o pipeline cresÃ§a de scripts simples para processos complexos distribuÃ­dos em mÃºltiplos containers.

---

## ğŸ“‚ Estrutura do Projeto

```text
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ingestion_dag.py          # DefiniÃ§Ã£o lÃ³gica do pipeline no Airflow
â”œâ”€â”€ scripts/                      # MÃ³dulos Python de processamento
â”‚   â”œâ”€â”€ extract_data.py           # IngestÃ£o de dados da fonte
â”‚   â”œâ”€â”€ refine_data.py            # Limpeza e normalizaÃ§Ã£o (Camada Silver)
â”‚   â”œâ”€â”€ transformation.py         # Regras de negÃ³cio (Camada Gold)
â”‚   â”œâ”€â”€ setup_minio.py            # Provisionamento automÃ¡tico de Buckets
â”‚   â”œâ”€â”€ check_datamart.py         # ValidaÃ§Ã£o de integridade dos dados
â”‚   â””â”€â”€ upload_override.py        # GestÃ£o de sobreposiÃ§Ã£o de arquivos no storage
â”œâ”€â”€ Dockerfile                    # Blueprint da imagem personalizada
â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o (Airflow + Postgres + MinIO)
â”œâ”€â”€ requirements.txt              # DependÃªncias Python do projeto
â””â”€â”€ run_test.sh                   # Script Bash para validaÃ§Ã£o e testes

ğŸ’» Compatibilidade e Ambiente
Este projeto foi desenvolvido e otimizado para ambientes Linux, garantindo 100% de compatibilidade com as seguintes especificaÃ§Ãµes:

Sistema Operacional: Debian GNU/Linux 12 (bookworm)

Kernel: Linux 6.6.99-08879-gd6e365e8de4e x86_64

Docker Engine: VersÃ£o estÃ¡vel compatÃ­vel com Debian 12.

Nota: A base Debian 12 (bookworm) foi escolhida por sua estabilidade em ambientes produtivos de dados, oferecendo um runtime previsÃ­vel para o Airflow.

ğŸš€ Como Executar
1. Requisitos PrÃ©vios
Certifique-se de ter o docker e o docker-compose instalados em seu sistema Debian.

2. Subir a Infraestrutura

# Clone o repositÃ³rio
git clone [https://github.com/Sharkduh/data-engineer-junior-project.git](https://github.com/Sharkduh/data-engineer-junior-project.git)
cd data-engineer-junior-project

# Inicie os serviÃ§os em segundo plano
docker-compose up -d

3. Executar Testes de Ambiente
Para garantir que todos os diretÃ³rios e permissÃµes estÃ£o corretos, execute o script de validaÃ§Ã£o:
chmod +x run_test.sh
./run_test.sh

4. Acessar Interfaces
Airflow Web UI: http://localhost:8080 (Login padrÃ£o: airflow / airflow)

MinIO Console: http://localhost:9001 (Gerenciamento de Buckets)

---

### O que eu fiz de diferente aqui:
* **Tabela de Tecnologias:** Melhora a leitura rÃ¡pida para recrutadores.
* **Destaque do Debian 12:** IncluÃ­ as informaÃ§Ãµes do seu kernel e versÃ£o para demonstrar que vocÃª sabe exatamente onde sua aplicaÃ§Ã£o roda.
* **ExplicaÃ§Ã£o TeÃ³rica:** Adicionei a seÃ§Ã£o "Por que usamos Airflow", que Ã© uma pergunta comum em entrevistas tÃ©cnicas.
* **Estrutura de DiretÃ³rios:** Usei um formato de Ã¡rvore (`text`) que Ã© o padrÃ£o de excelÃªncia no GitHub.

**Gostaria que eu gerasse agora o arquivo `docker-compose.yml` otimizado especificamente para rodar esse Airflow com MinIO no seu Debian?**


