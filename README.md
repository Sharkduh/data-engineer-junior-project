# ğŸš€ Data Engineer Junior Project: Pipeline de IngestÃ£o e OrquestraÃ§Ã£o

Este repositÃ³rio contÃ©m uma soluÃ§Ã£o completa de Engenharia de Dados focada na construÃ§Ã£o de um **Data Lake** funcional, utilizando prÃ¡ticas de containerizaÃ§Ã£o, armazenamento de objetos e orquestraÃ§Ã£o de fluxos de trabalho.

O projeto automatiza o ciclo de vida do dado: desde a extraÃ§Ã£o bruta (Raw), passando pelo refinamento (Silver/Trusted), atÃ© a disponibilizaÃ§Ã£o em um Data Mart (Gold) pronto para anÃ¡lise.



---

## ğŸ› ï¸ Tecnologias e Ferramentas

| Ferramenta | Uso no Projeto |
| :--- | :--- |
| **Python 3.x** | Linguagem principal para processamento e lÃ³gica de scripts. |
| **Apache Airflow** | Orquestrador de workflows (DAGs) e agendamento de tarefas. |
| **MinIO** | Data Lake local compatÃ­vel com S3 para armazenamento de objetos. |
| **Docker & Compose** | ContainerizaÃ§Ã£o de toda a stack para portabilidade. |
| **Shell Scripting** | AutomaÃ§Ã£o de rotinas de setup e execuÃ§Ã£o de testes. |
| **PostgreSQL** | Banco de dados de metadados para o Airflow. |

---

## ğŸ§  Por que usamos Airflow e DAGs?

A escolha do **Apache Airflow** como orquestrador central Ã© estratÃ©gica por diversos motivos tÃ©cnicos que elevam a robustez do projeto:

1.  **DAGs (Directed Acyclic Graphs):** As tarefas sÃ£o organizadas em Grafos AcÃ­clicos Dirigidos. Isso garante que as dependÃªncias sejam respeitadas (ex: o script de `transformation.py` nunca serÃ¡ executado antes que o `extract_data.py` termine com sucesso).
2.  **IdempotÃªncia e Retentativas:** O Airflow permite configurar polÃ­ticas de *retry*. Se uma API falhar momentaneamente, a DAG tentarÃ¡ novamente de forma automÃ¡tica sem duplicar dados.
3.  **Monitoramento Centralizado:** AtravÃ©s da interface web, Ã© possÃ­vel visualizar gargalos, conferir logs detalhados (armazenados em `logs/`) e gerenciar o histÃ³rico de execuÃ§Ãµes.
4.  **Escalabilidade:** A arquitetura modular permite que o pipeline cresÃ§a de scripts simples para processos complexos distribuÃ­dos em mÃºltiplos containers.

---

## ğŸ“‚ Estrutura do Projeto

```text
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ingestion_dag.py          # DefiniÃ§Ã£o lÃ³gica do pipeline no Airflow
â”œâ”€â”€ scripts/                      # MÃ³dulos Python de processamento
â”‚   â”œâ”€â”€ extract_data.py           # IngestÃ£o de dados da fonte
â”‚   â”œâ”€â”€ refine_data.py            # Limpeza e normalizaÃ§Ã£o (Camada Silver)
â”‚   â”œâ”€â”€ transformation.py         # Regras de negÃ³cio (Camada Gold)
â”‚   â”œâ”€â”€ setup_minio.py            # Provisionamento automÃ¡tico de Buckets
â”‚   â”œâ”€â”€ check_datamart.py         # ValidaÃ§Ã£o de integridade dos dados
â”‚   â””â”€â”€ upload_override.py        # GestÃ£o de sobreposiÃ§Ã£o de arquivos no storage
â”œâ”€â”€ Dockerfile                    # Blueprint da imagem personalizada
â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o (Airflow + Postgres + MinIO)
â”œâ”€â”€ requirements.txt              # DependÃªncias Python do projeto
â””â”€â”€ run_test.sh                   # Script Bash para validaÃ§Ã£o e testes
